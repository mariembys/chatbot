import streamlit as st
import os
from dotenv import load_dotenv
# Charger les variables d'environnement
load_dotenv()
# Importation des utilitaires LLM (Gemini) depuis le module
from rag_core import llm_utils 
from rag_core.anomaly_detector import AnomalyDetector
# --- Les futures imports pour le RAG (db_manager) viendront ici ---
from rag_core import db_manager 

# def main():
#     """
#     Fonction principale de l'application Streamlit.
#     GÃ¨re l'interface utilisateur et le flux de traitement de la requÃªte.
#     """
    
#     # 1. Configuration de la page
#     st.set_page_config(
#         page_title="Agent Commercial de Voyage IA - RAG",
#         page_icon="ğŸ¤–",
#         layout="wide" # Utilise toute la largeur de l'Ã©cran
#     )

#     st.title("âœˆï¸ Votre Agent Commercial de Voyage IA Multilingue")
#     st.markdown("""
#     Bienvenue ! Posez votre question concernant les voyages, en **FranÃ§ais**, en **Anglais**, en **Arabe** ou en **Dialecte Tunisien** (Derja).
#     """)

# # --- SIDEBAR : Configuration ---
#     with st.sidebar:
#         st.header("âš™ï¸ Configuration du SystÃ¨me")
        
#         st.markdown("### ğŸ“Š Ã‰tape 3 : PrÃ©parer le Dataset")
#         st.info("""
#         Chargez vos documents de voyage depuis le dossier `data/` 
#         et crÃ©ez la base vectorielle ChromaDB.
        
#         â±ï¸ PremiÃ¨re exÃ©cution : peut prendre 5-10 minutes 
#         (tÃ©lÃ©chargement du modÃ¨le LaBSE ~500 MB)
#         """)
        
#         # BOUTON POUR LANCER L'Ã‰TAPE 3
#         if st.button("ğŸš€ PrÃ©parer le Dataset", type="primary", use_container_width=True):
#             db_manager.pipeline_complet_preparation_dataset()




#     st.divider()
# # Afficher l'Ã©tat de la base vectorielle
#     st.markdown("### ğŸ“ˆ Ã‰tat du SystÃ¨me")
#     if os.path.exists("vectorstore/chroma_db"):
#         st.success("âœ… Base vectorielle crÃ©Ã©e")
#     else:
#         st.warning("âš ï¸ Base non crÃ©Ã©e")
#     st.divider()



#     # 2. Zone de Saisie de la RequÃªte Client
#     requete_client = st.text_area(
#         "Entrez votre requÃªte ici :",
#         height=150,
#         placeholder="Ex: Ù†Ø­Ø¨ Ù†Ø³Ø§ÙØ± Ù„ØªÙˆÙ†Ø³ ÙÙŠ Ø§Ù„ØµÙŠÙ. | I want to book a flight to Paris. | Je cherche des infos sur la visa pour DubaÃ¯."
#     )

#     # 3. Bouton de Soumission et DÃ©clenchement du Pipeline
#     if st.button("Chercher l'Information", type="primary"):
        
#         # VÃ©rification de l'input
#         if not requete_client:
#             st.warning("Veuillez entrer une requÃªte pour commencer.")
#             return # ArrÃªter l'exÃ©cution si la requÃªte est vide
            
#         # --- Ã‰TAPE 1 : PrÃ©paration ---
#         st.info(f"RequÃªte initiale : **{requete_client}**")
        
#         # Initialisation du client Gemini (la fonction vÃ©rifie la clÃ© API)
#         gemini_client = llm_utils.get_gemini_client()
        
#         st.divider()
        
#         # --- Ã‰TAPE 2 : Traitement Multilingue et Normalisation ---
#         with st.spinner("â³ Ã‰tape 2: Traduction et normalisation de la requÃªte (Gemini)..."):
#             requete_normalisee = llm_utils.traiter_requete_multilingue(
#                 gemini_client, requete_client
#             )

#         if requete_normalisee:
#             st.success("âœ… Ã‰TAPE 2 RÃ‰USSIE : RequÃªte normalisÃ©e (en FranÃ§ais) :")
#             st.code(requete_normalisee, language='text')

            
            

# if __name__ == "__main__":
#     main()
# chatbot.py (Fonction main)

import streamlit as st
import os
from dotenv import load_dotenv
# Charger les variables d'environnement
load_dotenv()

# Importation des utilitaires LLM (Gemini)
from rag_core import llm_utils 
# Importation du gestionnaire de base de donnÃ©es (RAG Indexing)
from rag_core import db_manager 
# NOUVEAU : Importation du DÃ©tecteur d'Anomalies
from rag_core.anomaly_detector import AnomalyDetector 

def main():
    """
    Fonction principale de l'application Streamlit.
    GÃ¨re l'interface utilisateur et le flux de traitement de la requÃªte.
    """
    
    # 1. Configuration de la page
    st.set_page_config(
        page_title="Agent Commercial de Voyage IA - RAG",
        page_icon="ğŸ¤–",
        layout="wide"
    )

    st.title("âœˆï¸ Votre Agent Commercial de Voyage IA Multilingue")
    st.markdown("""
    Bienvenue ! Posez votre question concernant les voyages, en **FranÃ§ais**, en **Anglais**, en **Arabe** ou en **Dialecte Tunisien** (Derja).
    """)

# --- SIDEBAR : Configuration et Initialisation ---
    with st.sidebar:
        st.header("âš™ï¸ Configuration du SystÃ¨me")
        
        st.markdown("### ğŸ“Š Ã‰tape 3 : PrÃ©parer le Dataset")
        st.info("""
        Chargez vos documents de voyage et crÃ©ez la base vectorielle ChromaDB.
        """)
        
        # BOUTON POUR LANCER L'Ã‰TAPE 3
        if st.button("ğŸš€ PrÃ©parer le Dataset", type="primary", use_container_width=True):
            vectorstore = db_manager.pipeline_complet_preparation_dataset()
            
            # Stockage de la base vectorielle et du dÃ©tecteur dans l'Ã©tat de session Streamlit
            if vectorstore:
                st.session_state['vectorstore'] = vectorstore
                # NOUVEAU : Initialiser le dÃ©tecteur aprÃ¨s la crÃ©ation de la base
                st.session_state['anomaly_detector'] = AnomalyDetector(vectorstore)


    st.divider()
    
# --- Chargement et Affichage de l'Ã‰tat du SystÃ¨me ---

    # Tenter de charger l'index RAG et le dÃ©tecteur au premier chargement de la page
    if 'vectorstore' not in st.session_state and os.path.exists(db_manager.VECTOR_STORE_PATH):
        with st.spinner("â³ Chargement de l'index de voyage existant..."):
            vectorstore = db_manager.load_existing_vector_store()
            st.session_state['vectorstore'] = vectorstore
            
            if vectorstore:
                st.session_state['anomaly_detector'] = AnomalyDetector(vectorstore) # Initialisation du dÃ©tecteur
    
    # Afficher l'Ã©tat de la base vectorielle
    if st.session_state.get('vectorstore'):
        st.success("âœ… Base vectorielle & DÃ©tecteur d'anomalies chargÃ©s.")
    else:
        st.warning("âš ï¸ Base de donnÃ©es non crÃ©Ã©e. Veuillez cliquer sur 'PrÃ©parer le Dataset'.")

    st.divider()



    # 2. Zone de Saisie de la RequÃªte Client
    requete_client = st.text_area(
        "Entrez votre requÃªte ici :",
        height=150,
        placeholder="Ex: Ù†Ø­Ø¨ Ù†Ø³Ø§ÙØ± Ù„ØªÙˆÙ†Ø³ ÙÙŠ Ø§Ù„ØµÙŠÙ. | I want to book a flight to Paris. | Je cherche des infos sur la visa pour DubaÃ¯."
    )

    # 3. Bouton de Soumission et DÃ©clenchement du Pipeline
    if st.button("Chercher l'Information", type="primary"):
        
        # VÃ©rification des prÃ©requis RAG
        if not st.session_state.get('vectorstore'):
            st.error("Impossible de chercher : la base de donnÃ©es vectorielle n'est pas chargÃ©e.")
            return

        # VÃ©rification de l'input
        if not requete_client:
            st.warning("Veuillez entrer une requÃªte pour commencer.")
            return 
            
        # --- Ã‰TAPE 1 : PrÃ©paration ---
        st.info(f"RequÃªte initiale : **{requete_client}**")
        
        # Initialisation du client Gemini (la fonction vÃ©rifie la clÃ© API)
        gemini_client = llm_utils.get_gemini_client()
        
        st.divider()
        
        # --- Ã‰TAPE 2 : Traitement Multilingue et Normalisation ---
        with st.spinner("â³ Ã‰tape 2: Traduction et normalisation de la requÃªte (Gemini)..."):
            requete_normalisee = llm_utils.traiter_requete_multilingue(
                gemini_client, requete_client
            )

        if requete_normalisee:
            st.success("âœ… Ã‰TAPE 2 RÃ‰USSIE : RequÃªte normalisÃ©e (en FranÃ§ais) :")
            st.code(requete_normalisee, language='text')
            
            st.divider()

            # --- NOUVEAU BLOC : ContrÃ´le du Sujet (Isolation Forest) ---
            st.markdown("### ğŸ” ContrÃ´le de Pertinence du Sujet")
            detector = st.session_state['anomaly_detector']
            
            with st.spinner("â³ VÃ©rification du sujet de la requÃªte (Isolation Forest)..."):
                is_outlier = detector.is_anomaly(requete_normalisee)
                
            if is_outlier:
                # Si l'anomalie est dÃ©tectÃ©e, nous arrÃªtons le RAG et affichons le message
                st.error("ğŸš« SUJET HORS-CONTRÃ”LE : Votre requÃªte est jugÃ©e hors sujet par le systÃ¨me. Veuillez vous concentrer uniquement sur les destinations, coÃ»ts, ou types de voyages prÃ©sents dans notre dataset.")
                return 
            else:
                st.success("âœ… Sujet pertinent dÃ©tectÃ©. Lancement du RAG.")
            # --------------------------------------------------------
            
            st.divider()
            
            # --- Ã‰TAPE 3 : Recherche RAG (Retrieval) ---
            st.markdown("### ğŸ” Ã‰TAPE 3 : Recherche de Contexte")
            vectorstore = st.session_state['vectorstore']
            
            with st.spinner("â³ Recherche de contexte pertinent dans la base de donnÃ©es..."):
                contexte_trouve = db_manager.search_db(
                    requete_normalisee, 
                    vectorstore,
                    k=3
                )
            
            if contexte_trouve:
                st.success("âœ… Contexte(s) rÃ©cupÃ©rÃ©(s) :")
                st.code(contexte_trouve, language='markdown')

                st.divider()

                # --- Ã‰TAPE 4 : GÃ©nÃ©ration AugmentÃ©e (Generation) ---
                st.markdown("### ğŸ’¬ Ã‰TAPE 4 : GÃ©nÃ©ration AugmentÃ©e")
                
                with st.spinner("â³ GÃ©nÃ©ration de la rÃ©ponse finale avec Gemini..."):
                    reponse_finale = llm_utils.generer_reponse_rag(
                        gemini_client, 
                        requete_normalisee, 
                        contexte_trouve
                    )
                
                if reponse_finale:
                    st.success("ğŸ¤– RÃ©ponse de l'Agent IA :")
                    st.markdown(reponse_finale) 
                else:
                    st.error("La gÃ©nÃ©ration de la rÃ©ponse finale a Ã©chouÃ©.")
            else:
                 st.warning("âš ï¸ Aucun contexte pertinent trouvÃ©. La rÃ©ponse sera gÃ©nÃ©rale ou basÃ©e sur un contexte vide.")
                # Si aucun contexte, on pourrait fallback sur une rÃ©ponse LLM pure


if __name__ == "__main__":
    main()